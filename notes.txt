$ srun --partition=common --time 10 --gres=gpu:1 -- nvidia-smi
Fri Apr 25 14:06:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.57.01              Driver Version: 565.57.01      CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA TITAN V                 On  |   00000000:03:00.0 Off |                  N/A |
| 31%   45C    P0             38W /  250W |       1MiB /  12288MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+


$ srun --partition=common --time 10 --gres=gpu:1 -- nvidia-smi --query-gpu=compute_cap --format=csv
compute_cap
7.0

https://docs.nvidia.com/cuda/volta-tuning-guide/index.html
https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf

Architecture: Volta (CC 7.0)
unified data cache size: 128 KB
shared mem capacity can be set to 0, 8, 16, 32, 64, 96 KB
Volta, max no of concurrent warps per SM: 64
register file size is 64k 32-bit registers per SM
max registers per thread is 255
max no of thread blocks per SM is 32
shared mem cap per sm is 96KB
tensor core operations A x B + C for A, B: 4x4 matrices as FP16

memory:
up to 8 memory dies per HBM2 stack, 4 stacks, max 32 

Tesla V100
6 GPCs, GPU Processing Clusters
each GPC have:
- 7 TPCs, Texture Processing Clusters
- 14 SMs, Streaming Multiprocessors
84 Volta SMs
each SM have:
- 64 FP32 cores
- 64 INT32 cores
- 32 FP64 cores
- 8 Tensor cores
- 4 Texture units
8 512-bit memory controllers

threads per warp: 32
warps per sm: 64
threads per sm: 2048
blocks per sm: 32
32b regs per sm: 65536
regs per block: 65536
regs per thread: 255
threads per block: 1024
fp32 cores per sm: 64
shmem size per sm: up to 96KB

page 18: physical layout


$ scontrol show nodes | grep Gres
   Gres=gpu:a100:8
   Gres=gpu:titanv:8
   Gres=gpu:rtx2080ti:6
   Gres=gpu:titanv:2,gpu:titanx:6
   Gres=gpu:rtx2080ti:2,gpu:titanx:6
   Gres=gpu:rtx2080ti:2,gpu:titanx:6
   Gres=gpu:rtx2080ti:8
   Gres=gpu:a6000:2
   Gres=gpu:h100:8
   Gres=gpu:titanv:4
   Gres=gpu:titanv:4

$ srun --partition=common --time 10 --gres=gpu:rtx2080ti -- nvidia-smi -q | grep Architecture
    Product Architecture                  : Turing

$ srun --partition=common --time 10 --gres=gpu:rtx2080ti --  lspci | grep -i vga
03:00.0 VGA compatible controller: NVIDIA Corporation TU102 [GeForce RTX 2080 Ti] (rev a1)

Informacje z techup stronki costam
RTX 2080 Ti = TU102 (TU102-300A-K1-A1)
Titan V = GV100 (GV100-400-A1)
Titan X = GP102 /// nie!GM200 (GM200-400-A1)???? nie! gp102

Cards available:
GeForce RTX 2080 Ti: 7.5 (Turing); chip TU102
- Tuning guide: https://docs.nvidia.com/cuda/pdf/Turing_Tuning_Guide.pdf
- TU102 whitepaper: https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf

NVIDIA TITAN V: 7.0 (Volta); chip GV100
- Tuning guide: https://docs.nvidia.com/cuda/archive/12.6.2/pdf/Volta_Tuning_Guide.pdf
- GV100 whitepaper: https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf

NVIDIA TITAN X: 6.1 (Pascal); chip GP102
- Tuning guide: https://docs.nvidia.com/cuda/pdf/Pascal_Tuning_Guide.pdf
- GP100 whitepaper: https://images.nvidia.com/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper.pdf 

Porównanie:
RTX ma memory bandwidth 616GB/s
Titan X: 480 GB/s
Titan V: 653 GB/s

Wybieram Titan V, bo różnica z RTX jest niewielka, a mamy 2x więcej dostępnych jednostek
RTX 2080 Ti:
- arch: Turing
- GPCs: 6
- TPCs: 34
- SMs: 68
- cores / SM: 64
- tensor cores / SM: 8
- RT cores: 68


dla komendy:
$ make rerun_worker 
rm tests/*_worker.out; TMPDIR=build srun --partition=common --time 10 --nodelist=asusgpu6 --gres=gpu:rtx2080ti -- make run_worker_parallel

// optymalizacje WORKER:
1. est. speedup 98,53%
The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 68 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the  Hardware Model description for more details on launch configurations.

przed: 2156877a8610d69433e78e7364f78d788e5fc588
./acotsp tests/euc2d-pr1002.tsp tests/euc2d-pr1002_worker.out WORKER 10 1 2 0.5 425
./acotsp tests/euc2d-pr1002.tsp tests/euc2d-pr1002_worker.out WORKER 10 1 2 0.5 425

=== Iteration Timing Statistics ===
Number of iterations: 10
Min time (ms): 4229.218
Max time (ms): 4265.509
Mean time (ms): 4246.988
Stddev time (ms): 12.247
===================================

./acotsp tests/euc2d-d1291.tsp tests/euc2d-d1291_worker.out WORKER 10 1 2 0.5 425

=== Iteration Timing Statistics ===
Number of iterations: 10
Min time (ms): 7299.107
Max time (ms): 7320.148
Mean time (ms): 7309.565
Stddev time (ms): 5.292
===================================

./acotsp tests/geo-gr96.tsp tests/geo-gr96_worker.out WORKER 10 1 2 0.5 425

=== Iteration Timing Statistics ===
Number of iterations: 10
Min time (ms): 23.335
Max time (ms): 23.691
Mean time (ms): 23.444
Stddev time (ms): 0.095
===================================

po: a4acfe7867f360d14f85ff650154a10165f8fa0f 
=== Iteration Timing Statistics ===
Number of iterations: 10
Min time (ms): 2942.690
Max time (ms): 2949.974
Mean time (ms): 2946.411
Stddev time (ms): 2.000
===================================

./acotsp tests/euc2d-d1291.tsp tests/euc2d-d1291_worker.out WORKER 10 1 2 0.5 425

=== Iteration Timing Statistics ===
Number of iterations: 10
Min time (ms): 5066.645
Max time (ms): 5074.497
Mean time (ms): 5071.095
Stddev time (ms): 2.000
===================================

./acotsp tests/geo-gr96.tsp tests/geo-gr96_worker.out WORKER 10 1 2 0.5 425

=== Iteration Timing Statistics ===
Number of iterations: 10
Min time (ms): 16.947
Max time (ms): 18.535
Mean time (ms): 17.198
Stddev time (ms): 0.452
===================================



Optimization: Shared memory! 97524b2a5c05a66050dbd5d0d209b82b62b1c21f
./acotsp tests/euc2d-pr1002.tsp tests/euc2d-pr1002_worker.out WORKER 10 1 2 0.5 425
Config: num_blocks: 68, tpb: 5, shmem: 45100

=== Iteration Timing Statistics ===
Number of iterations: 10
Min time (ms): 5905.676
Max time (ms): 5923.601
Mean time (ms): 5915.279
Stddev time (ms): 5.292
===================================

./acotsp tests/euc2d-d1291.tsp tests/euc2d-d1291_worker.out WORKER 10 1 2 0.5 425
Config: num_blocks: 68, tpb: 4, shmem: 46480

=== Iteration Timing Statistics ===
Number of iterations: 10
Min time (ms): 15738.025
Max time (ms): 15777.659
Mean time (ms): 15762.213
Stddev time (ms): 9.798
===================================

./acotsp tests/geo-gr96.tsp tests/geo-gr96_worker.out WORKER 10 1 2 0.5 425
Config: num_blocks: 68, tpb: 56, shmem: 48384

=== Iteration Timing Statistics ===
Number of iterations: 10
Min time (ms): 22.190
Max time (ms): 22.415
Mean time (ms): 22.345
Stddev time (ms): 0.068
===================================

